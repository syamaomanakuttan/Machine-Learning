{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaa3ee1c-3e1b-4e73-a843-47761e631345",
   "metadata": {},
   "source": [
    "## Comprehensive Assessment : Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf57fe4d-ea99-4730-90c2-e0e65edeb1fc",
   "metadata": {},
   "source": [
    "### Step 1: Load and Preprocess the Dataset\n",
    "\n",
    "Loading the Dataset:\n",
    "\n",
    "Load the dataset from the provided link.\n",
    "\n",
    "Inspect the dataset to understand its structure, including the data types, missing values, and basic statistics.\n",
    "\n",
    "Preprocessing:\n",
    "\n",
    "Handle missing values through imputation or removal, depending on the extent of the missing data.\n",
    "    \n",
    "Encode categorical variables using techniques such as one-hot encoding.\n",
    "\n",
    "Normalize or standardize the numerical features to ensure all features contribute equally to the model.\n",
    "    \n",
    "Split the dataset into training and testing sets to evaluate the performance of the models.\n",
    "    \n",
    "### Step 2: Implement Regression Models\n",
    "\n",
    "Implement the following regression algorithms:\n",
    "\n",
    "Linear Regression:\n",
    "\n",
    "A simple, interpretable model that assumes a linear relationship between the independent variables and the target variable (car price).\n",
    "\n",
    "Decision Tree Regressor:\n",
    "\n",
    "A non-linear model that splits the data into subsets based on feature values, building a tree structure.\n",
    "    \n",
    "Random Forest Regressor:\n",
    "\n",
    "An ensemble method that builds multiple decision trees and merges their results to improve performance and reduce overfitting.\n",
    "    \n",
    "Gradient Boosting Regressor:\n",
    "\n",
    "Another ensemble method that builds trees sequentially, with each tree trying to correct the errors of the previous one.\n",
    "    \n",
    "Support Vector Regressor:\n",
    "\n",
    "A model that uses the principles of support vector machines to perform regression, effective for high-dimensional spaces.\n",
    "                                                                                 \n",
    "### Step 3: Model Evaluation\n",
    "                                                                                 \n",
    "Metrics:\n",
    "                                                                                 \n",
    "Evaluate the models using R-squared, Mean Squared Error (MSE), and Mean Absolute Error (MAE).\n",
    "                                                                                 \n",
    "Compare these metrics across all models to identify the best performer.\n",
    "                                                                                 \n",
    "### Step 4: Feature Importance Analysis\n",
    "                                                                                 \n",
    "Feature Selection:\n",
    "                                                                                 \n",
    "Use methods such as feature importance from tree-based models or coefficients from linear models to identify significant variables.\n",
    "                                                                                 \n",
    "Visualize and interpret the importance of each feature in predicting car prices.\n",
    "                                                                                 \n",
    "### Step 5: Hyperparameter Tuning\n",
    "                                                                                 \n",
    "Tuning:\n",
    "                                                                                 \n",
    "Use techniques such as GridSearchCV or RandomizedSearchCV to perform hyperparameter tuning on the models.\n",
    "    \n",
    "Evaluate the performance of the tuned models and compare it to the default settings.\n",
    "\n",
    "\n",
    "Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a32a629f-ffb2-439b-9363-f5353e050daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             R-squared           MSE          MAE\n",
      "Linear Regression            -1.261189  1.785074e+08  7036.820982\n",
      "Decision Tree Regressor       0.878221  9.613756e+06  1932.662610\n",
      "Random Forest Regressor       0.955453  3.516732e+06  1334.409707\n",
      "Gradient Boosting Regressor   0.933338  5.262537e+06  1660.950927\n",
      "Support Vector Regressor     -0.099864  8.682769e+07  5695.713406\n",
      "Feature importances for Decision Tree Regressor:\n",
      "                feature  importance\n",
      "7            enginesize    0.648244\n",
      "6            curbweight    0.262414\n",
      "0                car_ID    0.018401\n",
      "14           highwaympg    0.014662\n",
      "4              carwidth    0.009319\n",
      "9                stroke    0.008328\n",
      "12              peakrpm    0.007991\n",
      "13              citympg    0.007263\n",
      "11           horsepower    0.004110\n",
      "84  CarName_peugeot 504    0.004049\n",
      "Feature importances for Random Forest Regressor:\n",
      "       feature  importance\n",
      "7   enginesize    0.579850\n",
      "6   curbweight    0.251657\n",
      "14  highwaympg    0.042796\n",
      "11  horsepower    0.030456\n",
      "0       car_ID    0.017557\n",
      "4     carwidth    0.015605\n",
      "2    wheelbase    0.009870\n",
      "3    carlength    0.007330\n",
      "9       stroke    0.005498\n",
      "12     peakrpm    0.005455\n",
      "Feature importances for Gradient Boosting Regressor:\n",
      "                                     feature  importance\n",
      "7                                 enginesize    0.596489\n",
      "6                                 curbweight    0.157753\n",
      "11                                horsepower    0.074433\n",
      "14                                highwaympg    0.061564\n",
      "0                                     car_ID    0.019676\n",
      "2                                  wheelbase    0.014854\n",
      "3                                  carlength    0.012978\n",
      "4                                   carwidth    0.011839\n",
      "164                      cylindernumber_four    0.010395\n",
      "29   CarName_buick regal sport coupe (turbo)    0.005112\n",
      "Best parameters found:  {'model__max_depth': 30, 'model__n_estimators': 300}\n",
      "Lowest RMSE found:  2476.6064421096307\n",
      "{'R-squared': 0.9552976671181101, 'MSE': 3528981.90039732, 'MAE': 1345.7493414634146}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Load the dataset\n",
    "url = 'https://drive.google.com/uc?id=1FHmYNLs9v0Enc-UExEMpitOFGsWvB2dP'\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Preprocessing\n",
    "# Handling missing values\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Splitting the dataset\n",
    "X = data.drop(columns=['price'])\n",
    "y = data['price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocessing pipelines for numerical and categorical data\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Model pipelines\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Decision Tree Regressor': DecisionTreeRegressor(),\n",
    "    'Random Forest Regressor': RandomForestRegressor(),\n",
    "    'Gradient Boosting Regressor': GradientBoostingRegressor(),\n",
    "    'Support Vector Regressor': SVR()\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('model', model)])\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    results[name] = {\n",
    "        'R-squared': r2_score(y_test, y_pred),\n",
    "        'MSE': mean_squared_error(y_test, y_pred),\n",
    "        'MAE': mean_absolute_error(y_test, y_pred)\n",
    "    }\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(results_df)\n",
    "\n",
    "# Feature Importance Analysis for tree-based models\n",
    "for name, model in models.items():\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                   ('model', model)])\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        feature_importances = model.feature_importances_\n",
    "        features = numeric_features.tolist() + pipeline.named_steps['preprocessor'].transformers_[1][1].named_steps['onehot'].get_feature_names_out(categorical_features).tolist()\n",
    "        feature_importance_df = pd.DataFrame({'feature': features, 'importance': feature_importances})\n",
    "        feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
    "        print(f'Feature importances for {name}:')\n",
    "        print(feature_importance_df.head(10))\n",
    "        \n",
    "# Hyperparameter Tuning example for Random Forest\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__max_depth': [10, 20, 30]\n",
    "}\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('model', RandomForestRegressor())])\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(-grid_search.best_score_))\n",
    "\n",
    "# Retrain the best model on the full training set and evaluate\n",
    "best_model = grid_search.best_estimator_\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "final_results = {\n",
    "    'R-squared': r2_score(y_test, y_pred),\n",
    "    'MSE': mean_squared_error(y_test, y_pred),\n",
    "    'MAE': mean_absolute_error(y_test, y_pred)\n",
    "}\n",
    "print(final_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da52c673-bac2-4af7-bd8e-c527d858908c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
